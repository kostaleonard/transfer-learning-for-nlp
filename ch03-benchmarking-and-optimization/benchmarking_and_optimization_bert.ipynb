{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8b33fb",
   "metadata": {},
   "source": [
    "# Benchmarking and optimization: BERT\n",
    "\n",
    "This notebook performs benchmarking using the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7531c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import email\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"mean\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable=True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path,\n",
    "            trainable=self.trainable,\n",
    "            name=f\"{self.name}_module\"\n",
    "        )\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [\n",
    "                var for var in trainable_vars if not \"/cls/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/ in var.name\"\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\"Undefined pooling type\")\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "        trainable_vars = [\n",
    "            var for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(\n",
    "                inputs=bert_inputs,\n",
    "                signature=\"tokens\",\n",
    "                as_dict=True\n",
    "            )[\"pooled_output\"]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(\n",
    "                inputs=bert_inputs,\n",
    "                signature=\"tokens\",\n",
    "                as_dict=True\n",
    "            )[\"sequence_output\"]\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(\n",
    "                mul_mask(x, m),\n",
    "                axis=1\n",
    "            ) / (tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(\"Undefined pooling type\")\n",
    "        return pooled\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_seq_length):\n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(\n",
    "        shape=(max_seq_length,),\n",
    "        name=\"segment_ids\"\n",
    "    )\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    bert_output = BertLayer(n_fine_tune_layers=0)(bert_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation=\"relu\")(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense)\n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e643f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(raw_data, header):\n",
    "    converted_data, labels = [], []\n",
    "    for i in range(raw_data.shape[0]):\n",
    "        out = \" \".join(raw_data[i])\n",
    "        converted_data.append(out)\n",
    "        labels.append(header[i])\n",
    "    converted_data = np.array(converted_data, dtype=object)[:, np.newaxis]\n",
    "    return converted_data, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def create_tokenizer_from_hub_module(bert_path):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module = hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [tokenization_info[\"vocab_file\"], tokenization_info[\"do_lower_case\"]]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be902a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "tokenizer = create_tokenizer_from_hub_module(bert_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dee6b",
   "metadata": {},
   "source": [
    "## Enron and fraudulent emails datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_filepath = \"../data/enron-email-dataset/emails.csv\"\n",
    "# We will preserve the typo in the filename as that is how it appears on Kaggle.\n",
    "fraud_filepath = \"../data/fraudulent-email-corpus/fradulent_emails.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0321e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = pd.read_csv(enron_filepath)\n",
    "emails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ccdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e468d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_messages(df):\n",
    "    messages = []\n",
    "    for item in df[\"message\"]:\n",
    "        e = email.message_from_string(item)\n",
    "        message_body = e.get_payload()\n",
    "        messages.append(message_body)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086354e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = extract_messages(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f4001",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_df = pd.DataFrame(bodies)\n",
    "bodies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98295892",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fraud_filepath, \"r\", encoding=\"latin1\") as infile:\n",
    "    data = infile.read()\n",
    "fraud_emails = data.split(\"From r\")\n",
    "len(fraud_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045cdeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_bodies = extract_messages(\n",
    "    pd.DataFrame(fraud_emails, columns=[\"message\"], dtype=str)\n",
    ")\n",
    "fraud_bodies_df = pd.DataFrame(fraud_bodies[1:])\n",
    "fraud_bodies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fraud_bodies_df[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamp = 1000\n",
    "maxtokens = 50\n",
    "maxtokenlen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    if row in [None, \"\"]:\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = str(row).split(\" \")[:maxtokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_expressions(row):\n",
    "    tokens = []\n",
    "    try:\n",
    "        for token in row:\n",
    "            token = token.lower()\n",
    "            token = re.sub(r\"[\\W\\d]\", \"\", token)\n",
    "            token = token[:maxtokenlen]\n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token = \"\"\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(row):\n",
    "    token = [token for token in row if token not in english_stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a69df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnronEmails = bodies_df.iloc[:, 0].apply(tokenize)\n",
    "EnronEmails = EnronEmails.apply(stop_word_removal)\n",
    "EnronEmails = EnronEmails.apply(reg_expressions)\n",
    "EnronEmails = EnronEmails.sample(Nsamp)\n",
    "SpamEmails = fraud_bodies_df.iloc[:, 0].apply(tokenize)\n",
    "SpamEmails = SpamEmails.apply(stop_word_removal)\n",
    "SpamEmails = SpamEmails.apply(reg_expressions)\n",
    "SpamEmails = SpamEmails.sample(Nsamp)\n",
    "raw_data = pd.concat((SpamEmails, EnronEmails), axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a52237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data.shape)\n",
    "print(raw_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054809c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Categories = [\"spam\", \"notspam\"]\n",
    "header = [1] * Nsamp + [0] * Nsamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffle_data(data, header):\n",
    "    p = np.random.permutation(len(header))\n",
    "    data = data[p]\n",
    "    header = np.asarray(header)[p]\n",
    "    return data, header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42895ae",
   "metadata": {},
   "source": [
    "### BERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, header = unison_shuffle_data(raw_data, header)\n",
    "idx = int(0.7 * raw_data.shape[0])\n",
    "train_x, train_y = convert_data(raw_data[:idx], header[:idx])\n",
    "test_x, test_y = convert_data(raw_data[idx:], header[idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeaee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = convert_text_to_examples(train_x, train_y)\n",
    "test_examples = convert_text_to_examples(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dc53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_input_ids,\n",
    "    train_input_masks,\n",
    "    train_segment_ids,\n",
    "    train_labels\n",
    ") = convert_examples_to_features(\n",
    "    tokenizer,\n",
    "    train_examples,\n",
    "    max_seq_length=maxtokens\n",
    ")\n",
    "(\n",
    "    test_input_ids,\n",
    "    test_input_masks,\n",
    "    test_segment_ids,\n",
    "    test_labels\n",
    ") = convert_examples_to_features(\n",
    "    tokenizer,\n",
    "    test_examples,\n",
    "    max_seq_length=maxtokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(maxtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eda390",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5765221",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids],\n",
    "    train_labels,\n",
    "    validation_data=(\n",
    "        [test_input_ids, test_input_masks, test_segment_ids],\n",
    "        test_labels\n",
    "    ),\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033ff03",
   "metadata": {},
   "source": [
    "## IMDB movie reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data, sentiments = [], []\n",
    "    for folder, sentiment in ((\"neg\", 0), (\"pos\", 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), \"r\") as reader:\n",
    "                text = reader.read()\n",
    "            text = tokenize(text)\n",
    "            text = stop_word_removal(text)\n",
    "            text = reg_expressions(text)\n",
    "            data.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "    data_np = np.array(data)\n",
    "    data, sentiments = unison_shuffle_data(data_np, sentiments)\n",
    "    return data, sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(\"..\", \"data\", \"aclImdb\", \"train\")\n",
    "raw_data, raw_header = load_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data.shape)\n",
    "print(len(raw_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffa5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(\n",
    "    range(len(raw_header)), size=(Nsamp * 2,),\n",
    "    replace=False\n",
    ")\n",
    "raw_data = raw_data[random_indices]\n",
    "raw_header = raw_header[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure roughly balanced class distribution.\n",
    "unique_elements, counts_elements = np.unique(raw_header, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d44258",
   "metadata": {},
   "source": [
    "### BERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b34554",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, raw_header = unison_shuffle_data(raw_data, raw_header)\n",
    "idx = int(0.7 * raw_data.shape[0])\n",
    "train_x, train_y = convert_data(raw_data[:idx], raw_header[:idx])\n",
    "test_x, test_y = convert_data(raw_data[idx:], raw_header[idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = convert_text_to_examples(train_x, train_y)\n",
    "test_examples = convert_text_to_examples(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b25b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_input_ids,\n",
    "    train_input_masks,\n",
    "    train_segment_ids,\n",
    "    train_labels\n",
    ") = convert_examples_to_features(\n",
    "    tokenizer,\n",
    "    train_examples,\n",
    "    max_seq_length=maxtokens\n",
    ")\n",
    "(\n",
    "    test_input_ids,\n",
    "    test_input_masks,\n",
    "    test_segment_ids,\n",
    "    test_labels\n",
    ") = convert_examples_to_features(\n",
    "    tokenizer,\n",
    "    test_examples,\n",
    "    max_seq_length=maxtokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889370cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(maxtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f883cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b69281",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids],\n",
    "    train_labels,\n",
    "    validation_data=(\n",
    "        [test_input_ids, test_input_masks, test_segment_ids],\n",
    "        test_labels\n",
    "    ),\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca76708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
